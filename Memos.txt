
Ch05


・zipは複数のリストをひとつにまとめる
・

・TODO
	irisでもcharacter transitionをやってみる
	
	for i を使って npのi番目の値を取り出す方法
	eigen_pairs = [np.abs(eigen_vals[i], eigen_vecs[:, i]) for i in range(len(eigen_vals))]
	課題 
		1)PCAで共分散行列にするメリットがわからない
		2)LDAの作成ロジックがよくわかっていない。
		3)PCAとLDAの違いがよくわからない。メリットとデメリット
	
・5.3 Kernel PCA

	(Keyword)
		Kernel-PCA
		Kernel trick


	線形分離できないデータを変換、線形分離に適した低次元の部分空間へ射影して分離する方法
	
	5.3.1 Kernel Trick
		高次元写像 => 再び低次元写像
		デメリット 計算量が大きい <= カーネルトリック = 変換行列は生成しない
		
		最もよく使用されるカーネル <= カーネルの使い分けは？？
			多項式カーネル
			双曲線正接カーネル
			動径基底関数(RBF) or ガウスカーネル
			
		カーネルトリックの手順
			1)カーネル行列Kの計算
			2)カーネル行列Kの中心化
			3)対応固有値に基づき中心化されたカーネル行列のk個の固有ベクトルを収集
			
		※PCAは教師なし学習で分散の最大化にクラスラベル情報をしようしない
		※新しいデータ点を射影するたびに、元のトレーニングセットデータを再利用しなければならない。
			なぜならカーネルPCAはメモリーベースな手法だから。＜＝どういう意味？？
			