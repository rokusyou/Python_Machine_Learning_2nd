
Ch05


・zipは複数のリストをひとつにまとめる
・

・TODO
	irisでもcharacter transitionをやってみる
	
	for i を使って npのi番目の値を取り出す方法
	eigen_pairs = [np.abs(eigen_vals[i], eigen_vecs[:, i]) for i in range(len(eigen_vals))]
	課題 
		1)PCAで共分散行列にするメリットがわからない
		2)LDAの作成ロジックがよくわかっていない。
		3)PCAとLDAの違いがよくわからない。メリットとデメリット
	
・5.3 Kernel PCA

	(Keyword)
		Kernel-PCA
		Kernel trick


	線形分離できないデータを変換、線形分離に適した低次元の部分空間へ射影して分離する方法
	
	5.3.1 Kernel Trick
		高次元写像 => 再び低次元写像
		デメリット 計算量が大きい <= カーネルトリック = 変換行列は生成しない
		
		最もよく使用されるカーネル <= カーネルの使い分けは？？
			多項式カーネル
			双曲線正接カーネル
			動径基底関数(RBF) or ガウスカーネル
			
		カーネルトリックの手順
			1)カーネル行列Kの計算
			2)カーネル行列Kの中心化
			3)対応固有値に基づき中心化されたカーネル行列のk個の固有ベクトルを収集
			
		※PCAは教師なし学習で分散の最大化にクラスラベル情報をしようしない
		※新しいデータ点を射影するたびに、元のトレーニングセットデータを再利用しなければならない。
			なぜならカーネルPCAはメモリーベースな手法だから。＜＝どういう意味？？
			
Ch06
	startified k-fold corss-validation
		各サブセットでクラスの比率を保持
		K-foldの大きなメリットとして計算処理を分散化できること！！
		
	6.5.2
		適合率 PRE
		再現率 REC
		F1-Score ※ PRE,RECの調和平均

Datasets
	ch06
	Breast Cancer Wisconsin
	AB desition
	https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data
	
	
	



TODO
	class transform
		Melon,Orange <=> 0,1
		One-hot transform
		
		
normaization => 最大値と最小値
stanardization => 平均値と標準偏差



バリアンスとバイアスの関係が理解不能だったが、学習不足か過学習の事だということが分かった。 例えば 「バリアンスとバイアスはトレードオフ」 の言い換えは 「過学習 = バリアンスが高い 、学習不足 = バイアスが高い」 or 「学習不足 = バリアンスが低い 、過学習 = バイアスが低い」



